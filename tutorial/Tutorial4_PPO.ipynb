{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO for Portfolio Management\n",
    "This tutorial is to demonstrate an example of using PPO to do portfolio management"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 18:16:02,932\tINFO worker.py:973 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "\n",
    "ROOT = os.path.dirname(os.path.abspath(\".\"))\n",
    "sys.path.append(ROOT)\n",
    "\n",
    "import argparse\n",
    "import os.path as osp\n",
    "from mmcv import Config\n",
    "from trademaster.utils import replace_cfg_vals\n",
    "from trademaster.nets.builder import build_net\n",
    "from trademaster.environments.builder import build_environment\n",
    "from trademaster.datasets.builder import build_dataset\n",
    "from trademaster.agents.builder import build_agent\n",
    "from trademaster.optimizers.builder import build_optimizer\n",
    "from trademaster.losses.builder import build_loss\n",
    "from trademaster.trainers.builder import build_trainer\n",
    "from trademaster.utils import plot\n",
    "from trademaster.utils import set_seed\n",
    "set_seed(2023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Import Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config (path: /Users/louison/PersonalTrade/configs/portfolio_management/portfolio_management_sz50_ppo_ppo_adam_mse.py): {'data': {'type': 'PortfolioManagementDataset', 'data_path': 'data/portfolio_management/sz50', 'train_path': 'data/portfolio_management/sz50/train.csv', 'valid_path': 'data/portfolio_management/sz50/valid.csv', 'test_path': 'data/portfolio_management/sz50/test.csv', 'tech_indicator_list': ['zopen', 'zhigh', 'zlow', 'zadjcp', 'zclose', 'zd_5', 'zd_10', 'zd_15', 'zd_20', 'zd_25', 'zd_30'], 'length_day': 10, 'initial_amount': 100000, 'transaction_cost_pct': 0.001, 'test_dynamic_path': 'data/portfolio_management/sz50/test.csv'}, 'environment': {'type': 'PortfolioManagementEnvironment'}, 'trainer': {'type': 'PortfolioManagementTrainer', 'agent_name': 'ppo', 'if_remove': False, 'configs': {'framework': 'tf2', 'num_workers': 0}, 'work_dir': 'work_dir/portfolio_management_sz50_ppo_ppo_adam_mse', 'epochs': 10}, 'loss': {'type': 'MSELoss'}, 'optimizer': {'type': 'Adam', 'lr': 0.001}, 'task_name': 'portfolio_management', 'dataset_name': 'sz50', 'net_name': 'ppo', 'agent_name': 'ppo', 'optimizer_name': 'adam', 'loss_name': 'mse', 'work_dir': 'work_dir/portfolio_management_sz50_ppo_ppo_adam_mse'}\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Download Alpaca Datasets')\n",
    "# parser.add_argument(\"--config\", default=osp.join(ROOT, \"configs\", \"portfolio_management\", \"portfolio_management_sse500_sarl_sarl_adam_mse.py\"),\n",
    "# parser.add_argument(\"--config\", default=osp.join(ROOT, \"configs\", \"portfolio_management\", \"portfolio_management_exchange_ppo_ppo_adam_mse.py\"),\n",
    "parser.add_argument(\"--config\", default=osp.join(ROOT, \"configs\", \"portfolio_management\", \"portfolio_management_sz50_ppo_ppo_adam_mse.py\"),\n",
    "                    help=\"download datasets config file path\")\n",
    "parser.add_argument(\"--task_name\", type=str, default=\"train\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "cfg = Config.fromfile(args.config)\n",
    "task_name = args.task_name\n",
    "cfg = replace_cfg_vals(cfg)\n",
    "print(cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kwargs': {'data_path': 'data/portfolio_management/sz50', 'train_path': 'data/portfolio_management/sz50/train.csv', 'valid_path': 'data/portfolio_management/sz50/valid.csv', 'test_path': 'data/portfolio_management/sz50/test.csv', 'tech_indicator_list': ['zopen', 'zhigh', 'zlow', 'zadjcp', 'zclose', 'zd_5', 'zd_10', 'zd_15', 'zd_20', 'zd_25', 'zd_30'], 'length_day': 10, 'initial_amount': 100000, 'transaction_cost_pct': 0.001, 'test_dynamic_path': 'data/portfolio_management/sz50/test.csv'}, 'data_path': '/Users/louison/PersonalTrade/data/portfolio_management/sz50', 'train_path': '/Users/louison/PersonalTrade/data/portfolio_management/sz50/train.csv', 'valid_path': '/Users/louison/PersonalTrade/data/portfolio_management/sz50/valid.csv', 'test_path': '/Users/louison/PersonalTrade/data/portfolio_management/sz50/test.csv', 'test_dynamic_path': '/Users/louison/PersonalTrade/data/portfolio_management/sz50/test.csv', 'tech_indicator_list': ['zopen', 'zhigh', 'zlow', 'zadjcp', 'zclose', 'zd_5', 'zd_10', 'zd_15', 'zd_20', 'zd_25', 'zd_30'], 'initial_amount': 100000, 'length_day': 10, 'transaction_cost_pct': 0.001}\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(cfg)\n",
    "print(vars(dataset))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Build Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 18:16:45,555\tINFO worker.py:973 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Arguments Keep work_dir: /Users/louison/PersonalTrade/work_dir/portfolio_management_exchange_ppo_ppo_adam_mse\n",
      "{'device': device(type='cpu'), 'configs': {'framework': 'tf2', 'num_workers': 0, 'env': 'portfolio_management', 'env_config': {'dataset': <trademaster.datasets.portfolio_management.dataset.PortfolioManagementDataset object at 0x1f3b4a2e0>, 'task': 'train'}}, 'agent_name': 'ppo', 'epochs': 10, 'dataset': <trademaster.datasets.portfolio_management.dataset.PortfolioManagementDataset object at 0x1f3b4a2e0>, 'work_dir': '/Users/louison/PersonalTrade/work_dir/portfolio_management_exchange_ppo_ppo_adam_mse', 'seeds_list': (12345,), 'random_seed': 12345, 'if_remove': False, 'num_threads': 8, 'trainer_name': <class 'ray.rllib.agents.ppo.ppo.PPOTrainer'>, 'checkpoints_path': '/Users/louison/PersonalTrade/work_dir/portfolio_management_exchange_ppo_ppo_adam_mse/checkpoints'}\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.registry import register_env\n",
    "import ray\n",
    "from trademaster.environments.portfolio_management.environment import PortfolioManagementEnvironment\n",
    "def env_creator(env_name):\n",
    "    if env_name == 'portfolio_management':\n",
    "        env = PortfolioManagementEnvironment\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return env\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "work_dir = os.path.join(ROOT, cfg.trainer.work_dir)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "register_env(\"portfolio_management\", lambda config: env_creator(\"portfolio_management\")(config))\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "cfg.dump(osp.join(work_dir, osp.basename(args.config)))\n",
    "\n",
    "trainer = build_trainer(cfg, default_args=dict(dataset=dataset, device = device))\n",
    "print(vars(trainer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: Train, Valid and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 18:16:47,463\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "         date     close                   tic      open      high       low   \n",
      "0  2000-01-04  0.656211     AUSTRALIAN DOLLAR  0.656211  0.656211  0.656211  \\\n",
      "0  2000-01-04  0.026932                  BAHT  0.026932  0.026932  0.026932   \n",
      "0  2000-01-04  0.688800       CANADIAN DOLLAR  0.688800  0.688800  0.688800   \n",
      "0  2000-01-04  0.138543          DANISH KRONE  0.138543  0.138543  0.138543   \n",
      "0  2000-01-04  1.030928                  EURO  1.030928  1.030928  1.030928   \n",
      "0  2000-01-04  0.642467                 FRANC  0.642467  0.642467  0.642467   \n",
      "0  2000-01-04  0.128576      HONG KONG DOLLAR  0.128576  0.128576  0.128576   \n",
      "0  2000-01-04  0.022962          INDIAN RUPEE  0.022962  0.022962  0.022962   \n",
      "0  2000-01-04  0.119617                 KRONA  0.119617  0.119617  0.119617   \n",
      "0  2000-01-04  0.105742          MEXICAN PESO  0.105742  0.105742  0.105742   \n",
      "0  2000-01-04  0.032680     NEW TAIWAN DOLLAR  0.032680  0.032680  0.032680   \n",
      "0  2000-01-04  0.519805     NEW ZELAND DOLLAR  0.519805  0.519805  0.519805   \n",
      "0  2000-01-04  0.126040       NORWEGIAN KRONE  0.126040  0.126040  0.126040   \n",
      "0  2000-01-04  0.164339                  RAND  0.164339  0.164339  0.164339   \n",
      "0  2000-01-04  0.543331                  REAL  0.543331  0.543331  0.543331   \n",
      "0  2000-01-04  0.263158               RINGGIT  0.263158  0.263158  0.263158   \n",
      "0  2000-01-04  0.604778      SINGAPORE DOLLAR  0.604778  0.604778  0.604778   \n",
      "0  2000-01-04  0.013765      SRI LANKAN RUPEE  0.013765  0.013765  0.013765   \n",
      "0  2000-01-04  1.636929  UNITED KINGDOM POUND  1.636929  1.636929  1.636929   \n",
      "0  2000-01-04  0.000891                   WON  0.000891  0.000891  0.000891   \n",
      "0  2000-01-04  0.009700                   YEN  0.009700  0.009700  0.009700   \n",
      "0  2000-01-04  0.120774                  YUAN  0.120774  0.120774  0.120774   \n",
      "\n",
      "      adjcp  zopen  zhigh  zlow  zadjcp    zclose      zd_5     zd_10   \n",
      "0  0.656211    0.0    0.0   0.0     0.0 -0.004397  0.001568 -0.011881  \\\n",
      "0  0.026932    0.0    0.0   0.0     0.0 -0.004309 -0.007849 -0.014487   \n",
      "0  0.688800    0.0    0.0   0.0     0.0 -0.003651 -0.009164 -0.011967   \n",
      "0  0.138543    0.0    0.0   0.0     0.0  0.015378 -0.008646 -0.011328   \n",
      "0  1.030928    0.0    0.0   0.0     0.0  0.015155 -0.005638 -0.007943   \n",
      "0  0.642467    0.0    0.0   0.0     0.0  0.015612 -0.005888 -0.008097   \n",
      "0  0.128576    0.0    0.0   0.0     0.0 -0.000129 -0.006636 -0.010106   \n",
      "0  0.022962    0.0    0.0   0.0     0.0  0.000000  0.000239  0.000041   \n",
      "0  0.119617    0.0    0.0   0.0     0.0  0.009928  0.000506  0.002181   \n",
      "0  0.105742    0.0    0.0   0.0     0.0 -0.005869 -0.002312 -0.004819   \n",
      "0  0.032680    0.0    0.0   0.0     0.0  0.025490 -0.001485 -0.002612   \n",
      "0  0.519805    0.0    0.0   0.0     0.0 -0.010656 -0.004917 -0.006340   \n",
      "0  0.126040    0.0    0.0   0.0     0.0  0.003781 -0.006944 -0.014386   \n",
      "0  0.164339    0.0    0.0   0.0     0.0  0.006738 -0.006254 -0.014264   \n",
      "0  0.543331    0.0    0.0   0.0     0.0 -0.019288 -0.006577 -0.013625   \n",
      "0  0.263158    0.0    0.0   0.0     0.0  0.000000 -0.006555 -0.009193   \n",
      "0  0.604778    0.0    0.0   0.0     0.0  0.001693 -0.006432 -0.009009   \n",
      "0  0.013765    0.0    0.0   0.0     0.0 -0.004818 -0.004567 -0.006165   \n",
      "0  1.636929    0.0    0.0   0.0     0.0  0.006057 -0.000386  0.000166   \n",
      "0  0.000891    0.0    0.0   0.0     0.0  0.004900 -0.013337 -0.015455   \n",
      "0  0.009700    0.0    0.0   0.0     0.0 -0.013483 -0.003300 -0.005555   \n",
      "0  0.120774    0.0    0.0   0.0     0.0 -0.000012 -0.004914 -0.005947   \n",
      "\n",
      "      zd_15     zd_20     zd_25     zd_30  \n",
      "0  0.008160  0.027933  0.028040  0.032544  \n",
      "0 -0.017411 -0.019852 -0.022889 -0.024430  \n",
      "0 -0.013248 -0.014619 -0.014858 -0.014843  \n",
      "0 -0.013422 -0.015202 -0.017156 -0.018115  \n",
      "0 -0.008732 -0.009855 -0.011481 -0.012016  \n",
      "0 -0.008790 -0.009926 -0.011564 -0.012078  \n",
      "0 -0.012558 -0.014629 -0.017614 -0.018756  \n",
      "0 -0.000700 -0.001780 -0.002401 -0.002761  \n",
      "0  0.004205  0.003219  0.001984  0.000628  \n",
      "0 -0.006041 -0.008935 -0.012234 -0.015154  \n",
      "0 -0.006020 -0.011368 -0.015860 -0.017684  \n",
      "0 -0.008488 -0.011090 -0.012779 -0.013905  \n",
      "0 -0.017905 -0.020950 -0.025943 -0.029813  \n",
      "0 -0.021033 -0.025878 -0.029377 -0.030881  \n",
      "0 -0.023323 -0.028963 -0.033060 -0.036293  \n",
      "0 -0.013223 -0.019279 -0.025654 -0.028553  \n",
      "0 -0.010963 -0.012755 -0.014268 -0.014699  \n",
      "0 -0.007019 -0.008400 -0.009798 -0.010270  \n",
      "0  0.000689  0.000581  0.000708  0.002275  \n",
      "0 -0.011215 -0.011441 -0.014749 -0.016799  \n",
      "0 -0.011356 -0.015907 -0.016542 -0.016013  \n",
      "0 -0.005263 -0.003942 -0.004124 -0.003351  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 18:16:47,892\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Episode: [1/10]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain_and_valid()\n",
      "File \u001b[0;32m~/PersonalTrade/trademaster/trainers/portfolio_management/trainer.py:112\u001b[0m, in \u001b[0;36mPortfolioManagementTrainer.train_and_valid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    111\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrain Episode: [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs))\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    113\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dataset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_environment \u001b[39m=\u001b[39m env_creator(\n\u001b[1;32m    115\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mportfolio_management\u001b[39m\u001b[39m\"\u001b[39m)(config)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/tune/trainable.py:360\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warmup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[1;32m    359\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 360\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    361\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mstep() needs to return a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m \u001b[39m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m step_ctx\u001b[39m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[1;32m   1110\u001b[0m     \u001b[39m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m         step_attempt_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_attempt()\n\u001b[1;32m   1113\u001b[0m     \u001b[39m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[39mexcept\u001b[39;00m RayError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1115\u001b[0m         \u001b[39m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[39m# No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evaluate_this_iter:\n\u001b[0;32m-> 1214\u001b[0m     step_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exec_plan_or_training_iteration_fn()\n\u001b[1;32m   1215\u001b[0m \u001b[39m# We have to evaluate in this training iteration.\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m     \u001b[39m# No parallelism.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mevaluation_parallel_to_training\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:2209\u001b[0m, in \u001b[0;36mTrainer._exec_plan_or_training_iteration_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2207\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2208\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39m_disable_execution_plan_api\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m-> 2209\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_iteration()\n\u001b[1;32m   2210\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2211\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/agents/ppo/ppo.py:437\u001b[0m, in \u001b[0;36mPPOTrainer.training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     train_batch \u001b[39m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    434\u001b[0m         worker_set\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers, max_agent_steps\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mtrain_batch_size\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     train_batch \u001b[39m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    438\u001b[0m         worker_set\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworkers, max_env_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mtrain_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    439\u001b[0m     )\n\u001b[1;32m    440\u001b[0m train_batch \u001b[39m=\u001b[39m train_batch\u001b[39m.\u001b[39mas_multi_agent()\n\u001b[1;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_batch\u001b[39m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/execution/rollout_ops.py:96\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mwhile\u001b[39;00m (max_agent_or_env_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m agent_or_env_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m     90\u001b[0m     max_agent_or_env_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[39mand\u001b[39;00m agent_or_env_steps \u001b[39m<\u001b[39m max_agent_or_env_steps\n\u001b[1;32m     92\u001b[0m ):\n\u001b[1;32m     93\u001b[0m     \u001b[39m# No remote workers in the set -> Use local worker for collecting\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[39m# samples.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m worker_set\u001b[39m.\u001b[39mremote_workers():\n\u001b[0;32m---> 96\u001b[0m         sample_batches \u001b[39m=\u001b[39m [worker_set\u001b[39m.\u001b[39;49mlocal_worker()\u001b[39m.\u001b[39;49msample()]\n\u001b[1;32m     97\u001b[0m     \u001b[39m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m         sample_batches \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mget(\n\u001b[1;32m    100\u001b[0m             [worker\u001b[39m.\u001b[39msample\u001b[39m.\u001b[39mremote() \u001b[39mfor\u001b[39;00m worker \u001b[39min\u001b[39;00m worker_set\u001b[39m.\u001b[39mremote_workers()]\n\u001b[1;32m    101\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:825\u001b[0m, in \u001b[0;36mRolloutWorker.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[39mif\u001b[39;00m log_once(\u001b[39m\"\u001b[39m\u001b[39msample_start\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    820\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGenerating sample batch of size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    821\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrollout_fragment_length\n\u001b[1;32m    822\u001b[0m         )\n\u001b[1;32m    823\u001b[0m     )\n\u001b[0;32m--> 825\u001b[0m batches \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_reader\u001b[39m.\u001b[39;49mnext()]\n\u001b[1;32m    826\u001b[0m steps_so_far \u001b[39m=\u001b[39m (\n\u001b[1;32m    827\u001b[0m     batches[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcount\n\u001b[1;32m    828\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount_steps_by \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39menv_steps\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    829\u001b[0m     \u001b[39melse\u001b[39;00m batches[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39magent_steps()\n\u001b[1;32m    830\u001b[0m )\n\u001b[1;32m    832\u001b[0m \u001b[39m# In truncate_episodes mode, never pull more than 1 batch per env.\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[39m# This avoids over-running the target batch size.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:115\u001b[0m, in \u001b[0;36mSamplerInput.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@override\u001b[39m(InputReader)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SampleBatchType:\n\u001b[0;32m--> 115\u001b[0m     batches \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_data()]\n\u001b[1;32m    116\u001b[0m     batches\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_extra_batches())\n\u001b[1;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batches) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:288\u001b[0m, in \u001b[0;36mSyncSampler.get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m@override\u001b[39m(SamplerInput)\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SampleBatchType:\n\u001b[1;32m    287\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m         item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_env_runner)\n\u001b[1;32m    289\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, RolloutMetrics):\n\u001b[1;32m    290\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics_queue\u001b[39m.\u001b[39mput(item)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:694\u001b[0m, in \u001b[0;36m_env_runner\u001b[0;34m(worker, base_env, extra_batch_callback, horizon, normalize_actions, clip_actions, multiple_episodes_in_batch, callbacks, perf_stats, soft_horizon, no_done_at_end, observation_fn, sample_collector, render)\u001b[0m\n\u001b[1;32m    692\u001b[0m t2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    693\u001b[0m \u001b[39m# types: Dict[PolicyID, Tuple[TensorStructType, StateBatch, dict]]\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m eval_results \u001b[39m=\u001b[39m _do_policy_eval(\n\u001b[1;32m    695\u001b[0m     to_eval\u001b[39m=\u001b[39;49mto_eval,\n\u001b[1;32m    696\u001b[0m     policies\u001b[39m=\u001b[39;49mworker\u001b[39m.\u001b[39;49mpolicy_map,\n\u001b[1;32m    697\u001b[0m     sample_collector\u001b[39m=\u001b[39;49msample_collector,\n\u001b[1;32m    698\u001b[0m     active_episodes\u001b[39m=\u001b[39;49mactive_episodes,\n\u001b[1;32m    699\u001b[0m )\n\u001b[1;32m    700\u001b[0m perf_stats\u001b[39m.\u001b[39minference_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t2\n\u001b[1;32m    702\u001b[0m \u001b[39m# Process results and update episode state.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py:1154\u001b[0m, in \u001b[0;36m_do_policy_eval\u001b[0;34m(to_eval, policies, sample_collector, active_episodes)\u001b[0m\n\u001b[1;32m   1151\u001b[0m         policy: Policy \u001b[39m=\u001b[39m _get_or_raise(policies, policy_id)\n\u001b[1;32m   1153\u001b[0m     input_dict \u001b[39m=\u001b[39m sample_collector\u001b[39m.\u001b[39mget_inference_input_dict(policy_id)\n\u001b[0;32m-> 1154\u001b[0m     eval_results[policy_id] \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mcompute_actions_from_input_dict(\n\u001b[1;32m   1155\u001b[0m         input_dict,\n\u001b[1;32m   1156\u001b[0m         timestep\u001b[39m=\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mglobal_timestep,\n\u001b[1;32m   1157\u001b[0m         episodes\u001b[39m=\u001b[39;49m[active_episodes[t\u001b[39m.\u001b[39;49menv_id] \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m eval_data],\n\u001b[1;32m   1158\u001b[0m     )\n\u001b[1;32m   1160\u001b[0m \u001b[39mif\u001b[39;00m log_once(\u001b[39m\"\u001b[39m\u001b[39mcompute_actions_result\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1161\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1162\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOutputs of compute_actions():\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(summarize(eval_results))\n\u001b[1;32m   1163\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/policy/eager_tf_policy.py:496\u001b[0m, in \u001b[0;36mbuild_eager_tf_policy.<locals>.eager_policy_cls.compute_actions_from_input_dict\u001b[0;34m(self, input_dict, explore, timestep, episodes, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39m# Call the exploration before_compute_actions hook.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration\u001b[39m.\u001b[39mbefore_compute_actions(\n\u001b[1;32m    493\u001b[0m     timestep\u001b[39m=\u001b[39mtimestep, explore\u001b[39m=\u001b[39mexplore, tf_sess\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_session()\n\u001b[1;32m    494\u001b[0m )\n\u001b[0;32m--> 496\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_actions_helper(\n\u001b[1;32m    497\u001b[0m     input_dict,\n\u001b[1;32m    498\u001b[0m     state_batches,\n\u001b[1;32m    499\u001b[0m     \u001b[39m# TODO: Passing episodes into a traced method does not work.\u001b[39;49;00m\n\u001b[1;32m    500\u001b[0m     \u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39meager_tracing\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39melse\u001b[39;49;00m episodes,\n\u001b[1;32m    501\u001b[0m     explore,\n\u001b[1;32m    502\u001b[0m     timestep,\n\u001b[1;32m    503\u001b[0m )\n\u001b[1;32m    504\u001b[0m \u001b[39m# Update our global timestep by the batch size.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_timestep\u001b[39m.\u001b[39massign_add(tree\u001b[39m.\u001b[39mflatten(ret[\u001b[39m0\u001b[39m])[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mas_list()[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/utils/threading.py:21\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m---> 21\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n\u001b[1;32m     22\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_lock\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/policy/eager_tf_policy.py:840\u001b[0m, in \u001b[0;36mbuild_eager_tf_policy.<locals>.eager_policy_cls._compute_actions_helper\u001b[0;34m(self, input_dict, state_batches, episodes, explore, timestep)\u001b[0m\n\u001b[1;32m    838\u001b[0m     dist_inputs, state_out, extra_fetches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(input_dict)\n\u001b[1;32m    839\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 840\u001b[0m     dist_inputs, state_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    841\u001b[0m         input_dict, state_batches, seq_lens\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    844\u001b[0m action_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_class(dist_inputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m    846\u001b[0m \u001b[39m# Get the exploration action from the forward results.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/models/modelv2.py:259\u001b[0m, in \u001b[0;36mModelV2.__call__\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    256\u001b[0m         restored[\u001b[39m\"\u001b[39m\u001b[39mobs_flat\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m input_dict[\u001b[39m\"\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    258\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext():\n\u001b[0;32m--> 259\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(restored, state \u001b[39mor\u001b[39;49;00m [], seq_lens)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(input_dict, SampleBatch):\n\u001b[1;32m    262\u001b[0m     input_dict\u001b[39m.\u001b[39maccessed_keys \u001b[39m=\u001b[39m restored\u001b[39m.\u001b[39maccessed_keys \u001b[39m-\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mobs_flat\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/models/tf/complex_input_net.py:185\u001b[0m, in \u001b[0;36mComplexInputNetwork.forward\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    180\u001b[0m         outs\u001b[39m.\u001b[39mappend(one_hot_out)\n\u001b[1;32m    181\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         nn_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten[i](\n\u001b[1;32m    183\u001b[0m             SampleBatch(\n\u001b[1;32m    184\u001b[0m                 {\n\u001b[0;32m--> 185\u001b[0m                     SampleBatch\u001b[39m.\u001b[39mOBS: tf\u001b[39m.\u001b[39;49mcast(\n\u001b[1;32m    186\u001b[0m                         tf\u001b[39m.\u001b[39;49mreshape(component, [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten_dims[i]]),\n\u001b[1;32m    187\u001b[0m                         tf\u001b[39m.\u001b[39;49mfloat32,\n\u001b[1;32m    188\u001b[0m                     )\n\u001b[1;32m    189\u001b[0m                 }\n\u001b[1;32m    190\u001b[0m             )\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m         outs\u001b[39m.\u001b[39mappend(nn_out)\n\u001b[1;32m    193\u001b[0m \u001b[39m# Concat all outputs and the non-image inputs.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1003\u001b[0m, in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m   1001\u001b[0m   x \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(x, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1002\u001b[0m   \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m base_type:\n\u001b[0;32m-> 1003\u001b[0m     x \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39;49mcast(x, base_type, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1004\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_complex \u001b[39mand\u001b[39;00m base_type\u001b[39m.\u001b[39mis_floating:\n\u001b[1;32m   1005\u001b[0m   logging\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mCasting complex to real discards imaginary part.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:1997\u001b[0m, in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   1995\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   1996\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1997\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   1998\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mCast\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x, \u001b[39m\"\u001b[39;49m\u001b[39mDstT\u001b[39;49m\u001b[39m\"\u001b[39;49m, DstT, \u001b[39m\"\u001b[39;49m\u001b[39mTruncate\u001b[39;49m\u001b[39m\"\u001b[39;49m, Truncate)\n\u001b[1;32m   1999\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   2000\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train_and_valid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rllib uses distributed training stategy and therefore bad result migh occurs during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from trademaster.environments.portfolio_management.environment import PortfolioManagementEnvironment\n",
    "def env_creator(env_name):\n",
    "    if env_name == 'portfolio_management':\n",
    "        env = PortfolioManagementEnvironment\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return env\n",
    "ray.init(ignore_reinit_error=True)\n",
    "register_env(\"portfolio_management\", lambda config: env_creator(\"portfolio_management\")(config))\n",
    "trainer.test();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(trainer.test_environment.save_asset_memory(),alg=\"PPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of 100 numbers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1f97403911abd3f02553c8f2b0c54537fddc7efadd9f5d3e31784db6e40c347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
