{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO for Portfolio Management\n",
    "This tutorial is to demonstrate an example of using PPO to do portfolio management"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:17:26,018\tINFO worker.py:973 -- Calling ray.init() again after it has already been called.\n",
      "[autoreload of trademaster.trainers.portfolio_management.reinforce_trainer failed: Traceback (most recent call last):\n",
      "  File \"/Users/louison/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/louison/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Users/louison/opt/anaconda3/envs/TradeMaster/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/Users/louison/PersonalTrade/trademaster/trainers/portfolio_management/reinforce_trainer.py\", line 55, in <module>\n",
      "    class PortfolioManagementREINFORCETrainer(Trainer):\n",
      "  File \"/Users/louison/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/mmcv/utils/registry.py\", line 337, in _register\n",
      "    self._register_module(module=module, module_name=name, force=force)\n",
      "  File \"/Users/louison/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/mmcv/utils/misc.py\", line 340, in new_func\n",
      "    output = old_func(*args, **kwargs)\n",
      "  File \"/Users/louison/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/mmcv/utils/registry.py\", line 272, in _register_module\n",
      "    raise KeyError(f'{name} is already registered '\n",
      "KeyError: 'PortfolioManagementREINFORCETrainer is already registered in trainer'\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "\n",
    "ROOT = os.path.dirname(os.path.abspath(\".\"))\n",
    "sys.path.append(ROOT)\n",
    "\n",
    "import argparse\n",
    "import os.path as osp\n",
    "from mmcv import Config\n",
    "from trademaster.utils import replace_cfg_vals\n",
    "from trademaster.nets.builder import build_net\n",
    "from trademaster.environments.builder import build_environment\n",
    "from trademaster.datasets.builder import build_dataset\n",
    "from trademaster.agents.builder import build_agent\n",
    "from trademaster.optimizers.builder import build_optimizer\n",
    "from trademaster.losses.builder import build_loss\n",
    "from trademaster.trainers.builder import build_trainer\n",
    "from trademaster.utils import plot\n",
    "from trademaster.utils import set_seed\n",
    "set_seed(2023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Import Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config (path: /Users/louison/PersonalTrade/configs/portfolio_management/portfolio_management_sz50_reinforce_reinforce_adam_mse.py): {'data': {'type': 'PortfolioManagementDataset', 'data_path': 'data/portfolio_management/sz50', 'train_path': 'data/portfolio_management/sz50/train.csv', 'valid_path': 'data/portfolio_management/sz50/valid.csv', 'test_path': 'data/portfolio_management/sz50/test.csv', 'tech_indicator_list': ['zopen', 'zhigh', 'zlow', 'zadjcp', 'zclose', 'zd_5', 'zd_10', 'zd_15', 'zd_20', 'zd_25', 'zd_30'], 'length_day': 10, 'initial_amount': 100000, 'transaction_cost_pct': 0.001, 'test_dynamic_path': 'data/portfolio_management/sz50/test.csv'}, 'environment': {'type': 'PortfolioManagementDeepTraderEnvironment'}, 'trainer': {'type': 'PortfolioManagementREINFORCETrainer', 'agent_name': 'pg', 'if_remove': False, 'configs': {'framework': 'tf2', 'num_workers': 0, 'rollout_fragment_length': 500, 'explore': True}, 'work_dir': 'work_dir/portfolio_management_sz50_deeptrader_pg_adam_mse', 'epochs': 1000}, 'loss': {'type': 'MSELoss'}, 'optimizer': {'type': 'Adam', 'lr': 0.001}, 'act_net': {'type': 'AssetScoringNet', 'N': None, 'K_l': None, 'num_inputs': None, 'num_channels': [12, 12, 12], 'kernel_size': 2, 'dropout': 0.2}, 'cri_net': {'type': 'AssetScoringValueNet', 'N': None, 'K_l': None, 'num_inputs': None, 'num_channels': [12, 12, 12], 'kernel_size': 2, 'dropout': 0.2}, 'market_net': {'type': 'MarketScoringNet', 'n_features': None, 'hidden_size': 12}, 'task_name': 'portfolio_management', 'dataset_name': 'sz50', 'net_name': 'deeptrader', 'agent_name': 'pg', 'optimizer_name': 'adam', 'loss_name': 'mse', 'work_dir': 'work_dir/portfolio_management_sz50_deeptrader_pg_adam_mse'}\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Download Alpaca Datasets')\n",
    "# parser.add_argument(\"--config\", default=osp.join(ROOT, \"configs\", \"portfolio_management\", \"portfolio_management_sse500_sarl_sarl_adam_mse.py\"),\n",
    "# parser.add_argument(\"--config\", default=osp.join(ROOT, \"configs\", \"portfolio_management\", \"portfolio_management_exchange_ppo_ppo_adam_mse.py\"),\n",
    "parser.add_argument(\"--config\", default=osp.join(ROOT, \"configs\", \"portfolio_management\", \"portfolio_management_sz50_sarl_sarl_adam_mse.py\"),\n",
    "# parser.add_argument(\"--config\", default=osp.join(ROOT, \"configs\", \"portfolio_management\", \"portfolio_management_dj30_deeptrader_deeptrader_adam_mse.py\"),\n",
    "# parser.add_argument(\"--config\", default=osp.join(ROOT, \"configs\", \"portfolio_management\", \"portfolio_management_sz50_reinforce_reinforce_adam_mse.py\"),\n",
    "                    help=\"download datasets config file path\")\n",
    "parser.add_argument(\"--task_name\", type=str, default=\"train\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "cfg = Config.fromfile(args.config)\n",
    "task_name = args.task_name\n",
    "cfg = replace_cfg_vals(cfg)\n",
    "print(cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kwargs': {'data_path': 'data/portfolio_management/sz50', 'train_path': 'data/portfolio_management/sz50/train.csv', 'valid_path': 'data/portfolio_management/sz50/valid.csv', 'test_path': 'data/portfolio_management/sz50/test.csv', 'tech_indicator_list': ['zopen', 'zhigh', 'zlow', 'zadjcp', 'zclose', 'zd_5', 'zd_10', 'zd_15', 'zd_20', 'zd_25', 'zd_30'], 'length_day': 10, 'initial_amount': 100000, 'transaction_cost_pct': 0.001, 'test_dynamic_path': 'data/portfolio_management/sz50/test.csv'}, 'data_path': '/Users/louison/PersonalTrade/data/portfolio_management/sz50', 'train_path': '/Users/louison/PersonalTrade/data/portfolio_management/sz50/train.csv', 'valid_path': '/Users/louison/PersonalTrade/data/portfolio_management/sz50/valid.csv', 'test_path': '/Users/louison/PersonalTrade/data/portfolio_management/sz50/test.csv', 'test_dynamic_path': '/Users/louison/PersonalTrade/data/portfolio_management/sz50/test.csv', 'tech_indicator_list': ['zopen', 'zhigh', 'zlow', 'zadjcp', 'zclose', 'zd_5', 'zd_10', 'zd_15', 'zd_20', 'zd_25', 'zd_30'], 'initial_amount': 100000, 'length_day': 10, 'transaction_cost_pct': 0.001}\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(cfg)\n",
    "print(vars(dataset))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Build Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:20:46,803\tINFO worker.py:973 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Arguments Keep work_dir: /Users/louison/PersonalTrade/work_dir/portfolio_management_sz50_deeptrader_pg_adam_mse\n",
      "{'device': device(type='cpu'), 'configs': {'framework': 'tf2', 'num_workers': 0, 'rollout_fragment_length': 500, 'explore': True, 'env': 'portfolio_management', 'env_config': {'dataset': <trademaster.datasets.portfolio_management.dataset.PortfolioManagementDataset object at 0x1f94aa9d0>, 'task': 'train', 'device': device(type='cpu')}}, 'agent_name': 'pg', 'epochs': 1000, 'dataset': <trademaster.datasets.portfolio_management.dataset.PortfolioManagementDataset object at 0x1f94aa9d0>, 'work_dir': '/Users/louison/PersonalTrade/work_dir/portfolio_management_sz50_deeptrader_pg_adam_mse', 'seeds_list': (12345,), 'random_seed': 12345, 'if_remove': False, 'num_threads': 8, 'trainer_name': <class 'ray.rllib.agents.pg.pg.PGTrainer'>, 'checkpoints_path': '/Users/louison/PersonalTrade/work_dir/portfolio_management_sz50_deeptrader_pg_adam_mse/checkpoints'}\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.registry import register_env\n",
    "import ray\n",
    "from trademaster.environments.portfolio_management.environment import PortfolioManagementEnvironment\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "work_dir = os.path.join(ROOT, cfg.trainer.work_dir)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "cfg.dump(osp.join(work_dir, osp.basename(args.config)))\n",
    "\n",
    "try:\n",
    "    del trainer\n",
    "except:\n",
    "    pass\n",
    "trainer = build_trainer(cfg, default_args=dict(dataset=dataset, device = device))\n",
    "print(vars(trainer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: Train, Valid and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 20:20:48,720\tWARNING deprecation.py:46 -- DeprecationWarning: `ray.rllib.agents.pg.default_config::DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.agents.pg.pg.PGConfig(...)` instead. This will raise an error in the future!\n",
      "2023-05-16 20:20:49,365\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2023-05-16 20:20:49,622\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Episode: [1/1000]\n",
      "Valid Episode: [1/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.573871%  |   1.090588  | 1.372496%  |  13.292012%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25580.296313\n",
      "Train Episode: [2/1000]\n",
      "Valid Episode: [2/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.878549%  |   1.101326  | 1.370761%  |  13.240382%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25892.219375\n",
      "Train Episode: [3/1000]\n",
      "Valid Episode: [3/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.817265%  |   1.099443  | 1.370674%  |  13.067875%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25830.992407\n",
      "Train Episode: [4/1000]\n",
      "Valid Episode: [4/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.817572%  |   1.097914  | 1.373068%  |  13.254117%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25811.176393\n",
      "Train Episode: [5/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "| -87.868240%  |  -1.005097  | 1.293616%  |  91.318704%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode: [5/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.844366%  |   1.100211  | 1.370811%  |  13.153288%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25873.110274\n",
      "Train Episode: [6/1000]\n",
      "Valid Episode: [6/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  28.019682%  |   1.105117  | 1.371774%  |  13.167595%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 26031.422175\n",
      "Train Episode: [7/1000]\n",
      "Valid Episode: [7/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.944894%  |   1.103669  | 1.370373%  |  13.097248%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25961.180452\n",
      "Train Episode: [8/1000]\n",
      "Valid Episode: [8/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.863093%  |   1.100687  | 1.370986%  |  13.179223%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25878.830871\n",
      "Train Episode: [9/1000]\n",
      "Valid Episode: [9/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.995631%  |   1.104000  | 1.372335%  |  13.229437%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 26010.934907\n",
      "Train Episode: [10/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "| -88.654871%  |  -1.050818  | 1.283202%  |  91.842291%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode: [10/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.784808%  |   1.097902  | 1.371471%  |  13.128096%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25784.999549\n",
      "Train Episode: [11/1000]\n",
      "Valid Episode: [11/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.761346%  |   1.097759  | 1.370543%  |  13.240095%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25775.146062\n",
      "Train Episode: [12/1000]\n",
      "Valid Episode: [12/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.618612%  |   1.092649  | 1.371479%  |  13.240181%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25626.712719\n",
      "Train Episode: [13/1000]\n",
      "Valid Episode: [13/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.486402%  |   1.089165  | 1.370389%  |  13.252015%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25502.288686\n",
      "Train Episode: [14/1000]\n",
      "Valid Episode: [14/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.685109%  |   1.094778  | 1.371439%  |  13.138791%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25694.645974\n",
      "Train Episode: [15/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "| -86.940113%  |  -0.966640  | 1.293853%  |  90.449380%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode: [15/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.598267%  |   1.092179  | 1.371198%  |  13.233084%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25612.872507\n",
      "Train Episode: [16/1000]\n",
      "Valid Episode: [16/1000]\n",
      "+--------------+-------------+------------+--------------+\n",
      "| Total Return | Sharp Ratio | Volatility | Max Drawdown |\n",
      "+--------------+-------------+------------+--------------+\n",
      "|  27.853235%  |   1.100160  | 1.371312%  |  13.275288%  |\n",
      "+--------------+-------------+------------+--------------+\n",
      "Valid Episode Reward Sum: 25858.679104\n",
      "Train Episode: [17/1000]\n",
      "Valid Episode: [17/1000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain_and_valid()\n",
      "File \u001b[0;32m~/PersonalTrade/trademaster/trainers/portfolio_management/reinforce_trainer.py:121\u001b[0m, in \u001b[0;36mPortfolioManagementREINFORCETrainer.train_and_valid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m episode_reward_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcompute_single_action(state, explore\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    122\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(action)\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mexp(action))\n\u001b[1;32m    123\u001b[0m     state, reward, done, information \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_environment\u001b[39m.\u001b[39mstep(\n\u001b[1;32m    124\u001b[0m         action)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1656\u001b[0m, in \u001b[0;36mTrainer.compute_single_action\u001b[0;34m(self, observation, state, prev_action, prev_reward, info, input_dict, policy_id, full_fetch, explore, timestep, episode, unsquash_action, clip_action, unsquash_actions, clip_actions, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m     action, state, extra \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mcompute_single_action(\n\u001b[1;32m   1649\u001b[0m         input_dict\u001b[39m=\u001b[39minput_dict,\n\u001b[1;32m   1650\u001b[0m         explore\u001b[39m=\u001b[39mexplore,\n\u001b[1;32m   1651\u001b[0m         timestep\u001b[39m=\u001b[39mtimestep,\n\u001b[1;32m   1652\u001b[0m         episode\u001b[39m=\u001b[39mepisode,\n\u001b[1;32m   1653\u001b[0m     )\n\u001b[1;32m   1654\u001b[0m \u001b[39m# Individual args.\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1656\u001b[0m     action, state, extra \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mcompute_single_action(\n\u001b[1;32m   1657\u001b[0m         obs\u001b[39m=\u001b[39;49mobservation,\n\u001b[1;32m   1658\u001b[0m         state\u001b[39m=\u001b[39;49mstate,\n\u001b[1;32m   1659\u001b[0m         prev_action\u001b[39m=\u001b[39;49mprev_action,\n\u001b[1;32m   1660\u001b[0m         prev_reward\u001b[39m=\u001b[39;49mprev_reward,\n\u001b[1;32m   1661\u001b[0m         info\u001b[39m=\u001b[39;49minfo,\n\u001b[1;32m   1662\u001b[0m         explore\u001b[39m=\u001b[39;49mexplore,\n\u001b[1;32m   1663\u001b[0m         timestep\u001b[39m=\u001b[39;49mtimestep,\n\u001b[1;32m   1664\u001b[0m         episode\u001b[39m=\u001b[39;49mepisode,\n\u001b[1;32m   1665\u001b[0m     )\n\u001b[1;32m   1667\u001b[0m \u001b[39m# If we work in normalized action space (normalize_actions=True),\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m \u001b[39m# we re-translate here into the env's action space.\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[39mif\u001b[39;00m unsquash_action:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/policy/policy.py:255\u001b[0m, in \u001b[0;36mPolicy.compute_single_action\u001b[0;34m(self, obs, state, prev_action, prev_reward, info, input_dict, episode, explore, timestep, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     episodes \u001b[39m=\u001b[39m [episode]\n\u001b[0;32m--> 255\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_actions_from_input_dict(\n\u001b[1;32m    256\u001b[0m     input_dict\u001b[39m=\u001b[39;49mSampleBatch(input_dict),\n\u001b[1;32m    257\u001b[0m     episodes\u001b[39m=\u001b[39;49mepisodes,\n\u001b[1;32m    258\u001b[0m     explore\u001b[39m=\u001b[39;49mexplore,\n\u001b[1;32m    259\u001b[0m     timestep\u001b[39m=\u001b[39;49mtimestep,\n\u001b[1;32m    260\u001b[0m )\n\u001b[1;32m    262\u001b[0m \u001b[39m# Some policies don't return a tuple, but always just a single action.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m# E.g. ES and ARS.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/policy/eager_tf_policy.py:496\u001b[0m, in \u001b[0;36mbuild_eager_tf_policy.<locals>.eager_policy_cls.compute_actions_from_input_dict\u001b[0;34m(self, input_dict, explore, timestep, episodes, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39m# Call the exploration before_compute_actions hook.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration\u001b[39m.\u001b[39mbefore_compute_actions(\n\u001b[1;32m    493\u001b[0m     timestep\u001b[39m=\u001b[39mtimestep, explore\u001b[39m=\u001b[39mexplore, tf_sess\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_session()\n\u001b[1;32m    494\u001b[0m )\n\u001b[0;32m--> 496\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_actions_helper(\n\u001b[1;32m    497\u001b[0m     input_dict,\n\u001b[1;32m    498\u001b[0m     state_batches,\n\u001b[1;32m    499\u001b[0m     \u001b[39m# TODO: Passing episodes into a traced method does not work.\u001b[39;49;00m\n\u001b[1;32m    500\u001b[0m     \u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39meager_tracing\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39melse\u001b[39;49;00m episodes,\n\u001b[1;32m    501\u001b[0m     explore,\n\u001b[1;32m    502\u001b[0m     timestep,\n\u001b[1;32m    503\u001b[0m )\n\u001b[1;32m    504\u001b[0m \u001b[39m# Update our global timestep by the batch size.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_timestep\u001b[39m.\u001b[39massign_add(tree\u001b[39m.\u001b[39mflatten(ret[\u001b[39m0\u001b[39m])[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mas_list()[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/utils/threading.py:21\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m---> 21\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n\u001b[1;32m     22\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_lock\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/policy/eager_tf_policy.py:840\u001b[0m, in \u001b[0;36mbuild_eager_tf_policy.<locals>.eager_policy_cls._compute_actions_helper\u001b[0;34m(self, input_dict, state_batches, episodes, explore, timestep)\u001b[0m\n\u001b[1;32m    838\u001b[0m     dist_inputs, state_out, extra_fetches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(input_dict)\n\u001b[1;32m    839\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 840\u001b[0m     dist_inputs, state_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    841\u001b[0m         input_dict, state_batches, seq_lens\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    844\u001b[0m action_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_class(dist_inputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m    846\u001b[0m \u001b[39m# Get the exploration action from the forward results.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/models/modelv2.py:259\u001b[0m, in \u001b[0;36mModelV2.__call__\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    256\u001b[0m         restored[\u001b[39m\"\u001b[39m\u001b[39mobs_flat\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m input_dict[\u001b[39m\"\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    258\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext():\n\u001b[0;32m--> 259\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(restored, state \u001b[39mor\u001b[39;49;00m [], seq_lens)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(input_dict, SampleBatch):\n\u001b[1;32m    262\u001b[0m     input_dict\u001b[39m.\u001b[39maccessed_keys \u001b[39m=\u001b[39m restored\u001b[39m.\u001b[39maccessed_keys \u001b[39m-\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mobs_flat\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/models/tf/complex_input_net.py:182\u001b[0m, in \u001b[0;36mComplexInputNetwork.forward\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    180\u001b[0m         outs\u001b[39m.\u001b[39mappend(one_hot_out)\n\u001b[1;32m    181\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m         nn_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten[i](\n\u001b[1;32m    183\u001b[0m             SampleBatch(\n\u001b[1;32m    184\u001b[0m                 {\n\u001b[1;32m    185\u001b[0m                     SampleBatch\u001b[39m.\u001b[39;49mOBS: tf\u001b[39m.\u001b[39;49mcast(\n\u001b[1;32m    186\u001b[0m                         tf\u001b[39m.\u001b[39;49mreshape(component, [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten_dims[i]]),\n\u001b[1;32m    187\u001b[0m                         tf\u001b[39m.\u001b[39;49mfloat32,\n\u001b[1;32m    188\u001b[0m                     )\n\u001b[1;32m    189\u001b[0m                 }\n\u001b[1;32m    190\u001b[0m             )\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m         outs\u001b[39m.\u001b[39mappend(nn_out)\n\u001b[1;32m    193\u001b[0m \u001b[39m# Concat all outputs and the non-image inputs.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/models/modelv2.py:259\u001b[0m, in \u001b[0;36mModelV2.__call__\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    256\u001b[0m         restored[\u001b[39m\"\u001b[39m\u001b[39mobs_flat\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m input_dict[\u001b[39m\"\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    258\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext():\n\u001b[0;32m--> 259\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(restored, state \u001b[39mor\u001b[39;49;00m [], seq_lens)\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(input_dict, SampleBatch):\n\u001b[1;32m    262\u001b[0m     input_dict\u001b[39m.\u001b[39maccessed_keys \u001b[39m=\u001b[39m restored\u001b[39m.\u001b[39maccessed_keys \u001b[39m-\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mobs_flat\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/models/tf/fcnet.py:146\u001b[0m, in \u001b[0;36mFullyConnectedNetwork.forward\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    141\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    142\u001b[0m     input_dict: Dict[\u001b[39mstr\u001b[39m, TensorType],\n\u001b[1;32m    143\u001b[0m     state: List[TensorType],\n\u001b[1;32m    144\u001b[0m     seq_lens: TensorType,\n\u001b[1;32m    145\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m (TensorType, List[TensorType]):\n\u001b[0;32m--> 146\u001b[0m     model_out, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(input_dict[\u001b[39m\"\u001b[39;49m\u001b[39mobs_flat\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    147\u001b[0m     \u001b[39mreturn\u001b[39;00m model_out, state\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/keras/engine/training.py:561\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    559\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/keras/engine/functional.py:511\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    493\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    494\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \n\u001b[1;32m    496\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/keras/engine/functional.py:668\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    667\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 668\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mlayer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    670\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    672\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[1;32m    673\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/keras/engine/base_layer.py:1066\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     input_list \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(inputs)\n\u001b[1;32m   1062\u001b[0m \u001b[39m# Handle `mask` propagation from previous layer to current layer. Masks\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[39m# can be propagated explicitly via the `mask` argument, or implicitly\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[39m# via setting the `_keras_mask` attribute on the inputs to a Layer.\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[39m# Masks passed explicitly take priority.\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m input_masks, mask_is_implicit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_input_masks(\n\u001b[1;32m   1067\u001b[0m     inputs, input_list, args, kwargs\n\u001b[1;32m   1068\u001b[0m )\n\u001b[1;32m   1069\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expects_mask_arg \u001b[39mand\u001b[39;00m mask_is_implicit:\n\u001b[1;32m   1070\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m input_masks\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/keras/engine/base_layer.py:2878\u001b[0m, in \u001b[0;36mLayer._get_input_masks\u001b[0;34m(self, inputs, input_list, args, kwargs)\u001b[0m\n\u001b[1;32m   2876\u001b[0m     implicit_mask \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2877\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2878\u001b[0m     input_masks \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(t, \u001b[39m\"\u001b[39m\u001b[39m_keras_mask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m input_list]\n\u001b[1;32m   2879\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m mask \u001b[39min\u001b[39;00m input_masks):\n\u001b[1;32m   2880\u001b[0m         input_masks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/keras/engine/base_layer.py:2878\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2876\u001b[0m     implicit_mask \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2877\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2878\u001b[0m     input_masks \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39;49m(t, \u001b[39m\"\u001b[39;49m\u001b[39m_keras_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m input_list]\n\u001b[1;32m   2879\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m mask \u001b[39min\u001b[39;00m input_masks):\n\u001b[1;32m   2880\u001b[0m         input_masks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train_and_valid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rllib uses distributed training stategy and therefore bad result migh occurs during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 23:49:47,947\tWARNING deprecation.py:46 -- DeprecationWarning: `config['prioritized_replay']` has been deprecated. Replay prioritization specified at new location config['replay_buffer_config']['prioritized_replay'] will be overwritten. This will raise an error in the future!\n",
      "2023-05-12 23:49:47,949\tWARNING deprecation.py:46 -- DeprecationWarning: `config[prioritized_replay_alpha]` has been deprecated. config['replay_buffer_config'][prioritized_replay_alpha] should be used for Q-Learning algorithms. Ignore this warning if you are not using a Q-Learning algorithm and still provide prioritized_replay_alpha. This will raise an error in the future!\n",
      "2023-05-12 23:49:47,950\tWARNING deprecation.py:46 -- DeprecationWarning: `config[prioritized_replay_beta]` has been deprecated. config['replay_buffer_config'][prioritized_replay_beta] should be used for Q-Learning algorithms. Ignore this warning if you are not using a Q-Learning algorithm and still provide prioritized_replay_beta. This will raise an error in the future!\n",
      "2023-05-12 23:49:47,951\tWARNING deprecation.py:46 -- DeprecationWarning: `config[prioritized_replay_eps]` has been deprecated. config['replay_buffer_config'][prioritized_replay_eps] should be used for Q-Learning algorithms. Ignore this warning if you are not using a Q-Learning algorithm and still provide prioritized_replay_eps. This will raise an error in the future!\n",
      "2023-05-12 23:49:47,952\tWARNING deprecation.py:46 -- DeprecationWarning: `config[learning_starts]` has been deprecated. config['replay_buffer_config'][learning_starts] should be used for Q-Learning algorithms. Ignore this warning if you are not using a Q-Learning algorithm and still provide learning_starts. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "EnvError",
     "evalue": "The env string you provided ('portfolio_management_sarl') is:\na) Not a supported/installed environment.\nb) Not a tune-registered environment creator.\nc) Not a valid env class string.\n\nTry one of the following:\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n   For VizDoom support: Install VizDoom\n   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n   `pip install vizdoomgym`.\n   For PyBullet support: `pip install pybullet`.\nb) To register your custom env, do `from ray import tune;\n   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n   Then in your config, do `config['env'] = [name]`.\nc) Make sure you provide a fully qualified classpath, e.g.:\n   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator)\n\u001b[1;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer._init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init\u001b[39m(\u001b[39mself\u001b[39m, config: TrainerConfigDict, env_creator: EnvCreator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/env/utils.py:54\u001b[0m, in \u001b[0;36mgym_env_creator\u001b[0;34m(env_context, env_descriptor)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m gym\u001b[39m.\u001b[39;49mmake(env_descriptor, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49menv_context)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m gym\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mError:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/gym/envs/registration.py:235\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake\u001b[39m(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 235\u001b[0m     \u001b[39mreturn\u001b[39;00m registry\u001b[39m.\u001b[39;49mmake(\u001b[39mid\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/gym/envs/registration.py:128\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mMaking new env: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, path)\n\u001b[0;32m--> 128\u001b[0m spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspec(path)\n\u001b[1;32m    129\u001b[0m env \u001b[39m=\u001b[39m spec\u001b[39m.\u001b[39mmake(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/gym/envs/registration.py:151\u001b[0m, in \u001b[0;36mEnvRegistry.spec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m match:\n\u001b[0;32m--> 151\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\n\u001b[1;32m    152\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to look up malformed environment ID: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. (Currently all IDs must be of the form \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    153\u001b[0m             \u001b[39mid\u001b[39m\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m), env_id_re\u001b[39m.\u001b[39mpattern\n\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mError\u001b[0m: Attempted to look up malformed environment ID: b'portfolio_management_sarl'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEnvError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m env\n\u001b[1;32m     10\u001b[0m \u001b[39m# ray.init(ignore_reinit_error=True)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# register_env(\"portfolio_management\", lambda config: env_creator(\"portfolio_management\")(config))\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m trainer\u001b[39m.\u001b[39;49mtest();\n",
      "File \u001b[0;32m~/PersonalTrade/trademaster/trainers/portfolio_management/sarl_trainer.py:133\u001b[0m, in \u001b[0;36mPortfolioManagementSARLTrainer.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer_name(env\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mportfolio_management_sarl\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfigs)\n\u001b[1;32m    135\u001b[0m     obj \u001b[39m=\u001b[39m load_object(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoints_path, \u001b[39m\"\u001b[39m\u001b[39mbest.pkl\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    136\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mrestore_from_object(obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:870\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    863\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     }\n\u001b[1;32m    868\u001b[0m }\n\u001b[0;32m--> 870\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    871\u001b[0m     config, logger_creator, remote_checkpoint_dir, sync_function_tpl\n\u001b[1;32m    872\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/tune/trainable.py:156\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    154\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 156\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    157\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:950\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 950\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    951\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    952\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    953\u001b[0m         policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    954\u001b[0m         trainer_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    955\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    956\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    957\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    958\u001b[0m     )\n\u001b[1;32m    959\u001b[0m     \u001b[39m# By default, collect metrics for all remote workers.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers_for_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mremote_workers()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:170\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    167\u001b[0m     spaces \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m local_worker:\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_worker \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_worker(\n\u001b[1;32m    171\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49mRolloutWorker,\n\u001b[1;32m    172\u001b[0m         env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[1;32m    173\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    174\u001b[0m         policy_cls\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_class,\n\u001b[1;32m    175\u001b[0m         worker_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    176\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    177\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_local_config,\n\u001b[1;32m    178\u001b[0m         spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[1;32m    179\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:630\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[0;34m(self, cls, env_creator, validate_env, policy_cls, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     extra_python_environs \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mextra_python_environs_for_worker\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 630\u001b[0m worker \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    631\u001b[0m     env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[1;32m    632\u001b[0m     validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    633\u001b[0m     policy_spec\u001b[39m=\u001b[39;49mpolicies,\n\u001b[1;32m    634\u001b[0m     policy_mapping_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicy_mapping_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    635\u001b[0m     policies_to_train\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicies_to_train\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    636\u001b[0m     tf_session_creator\u001b[39m=\u001b[39;49m(session_creator \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mtf_session_args\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    637\u001b[0m     rollout_fragment_length\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrollout_fragment_length\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    638\u001b[0m     count_steps_by\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mcount_steps_by\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    639\u001b[0m     batch_mode\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_mode\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    640\u001b[0m     episode_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mhorizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    641\u001b[0m     preprocessor_pref\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mpreprocessor_pref\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    642\u001b[0m     sample_async\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msample_async\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    643\u001b[0m     compress_observations\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcompress_observations\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    644\u001b[0m     num_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_envs_per_worker\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    645\u001b[0m     observation_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mobservation_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    646\u001b[0m     observation_filter\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mobservation_filter\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    647\u001b[0m     clip_rewards\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_rewards\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    648\u001b[0m     normalize_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnormalize_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    649\u001b[0m     clip_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    650\u001b[0m     env_config\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39menv_config\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    651\u001b[0m     policy_config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    652\u001b[0m     worker_index\u001b[39m=\u001b[39;49mworker_index,\n\u001b[1;32m    653\u001b[0m     num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    654\u001b[0m     recreated_worker\u001b[39m=\u001b[39;49mrecreated_worker,\n\u001b[1;32m    655\u001b[0m     record_env\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrecord_env\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    656\u001b[0m     log_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logdir,\n\u001b[1;32m    657\u001b[0m     log_level\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mlog_level\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    658\u001b[0m     callbacks\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    659\u001b[0m     input_creator\u001b[39m=\u001b[39;49minput_creator,\n\u001b[1;32m    660\u001b[0m     input_evaluation\u001b[39m=\u001b[39;49minput_evaluation,\n\u001b[1;32m    661\u001b[0m     output_creator\u001b[39m=\u001b[39;49moutput_creator,\n\u001b[1;32m    662\u001b[0m     remote_worker_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_worker_envs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    663\u001b[0m     remote_env_batch_wait_ms\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_env_batch_wait_ms\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    664\u001b[0m     soft_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msoft_horizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    665\u001b[0m     no_done_at_end\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mno_done_at_end\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    666\u001b[0m     seed\u001b[39m=\u001b[39;49m(config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m+\u001b[39;49m worker_index)\n\u001b[1;32m    667\u001b[0m     \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    668\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    669\u001b[0m     fake_sampler\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mfake_sampler\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    670\u001b[0m     extra_python_environs\u001b[39m=\u001b[39;49mextra_python_environs,\n\u001b[1;32m    671\u001b[0m     spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[1;32m    672\u001b[0m     disable_env_checking\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mdisable_env_checking\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    673\u001b[0m )\n\u001b[1;32m    675\u001b[0m \u001b[39mreturn\u001b[39;00m worker\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:506\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[0;34m(self, env_creator, validate_env, policy_spec, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, observation_filter, clip_rewards, normalize_actions, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, recreated_worker, record_env, log_dir, log_level, callbacks, input_creator, input_evaluation, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler, spaces, policy, monitor_path, disable_env_checking)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39m# Create a (single) env for this worker.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[1;32m    501\u001b[0m     worker_index \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    502\u001b[0m     \u001b[39mand\u001b[39;00m num_workers \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    503\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m policy_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcreate_env_on_driver\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    504\u001b[0m ):\n\u001b[1;32m    505\u001b[0m     \u001b[39m# Run the `env_creator` function passing the EnvContext.\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env_creator(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_context))\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    509\u001b[0m     \u001b[39m# Validate environment (general validation function).\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_env_checking:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TradeMaster/lib/python3.9/site-packages/ray/rllib/env/utils.py:56\u001b[0m, in \u001b[0;36mgym_env_creator\u001b[0;34m(env_context, env_descriptor)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m gym\u001b[39m.\u001b[39mmake(env_descriptor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39menv_context)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m gym\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mError:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mraise\u001b[39;00m EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR\u001b[39m.\u001b[39mformat(env_descriptor))\n",
      "\u001b[0;31mEnvError\u001b[0m: The env string you provided ('portfolio_management_sarl') is:\na) Not a supported/installed environment.\nb) Not a tune-registered environment creator.\nc) Not a valid env class string.\n\nTry one of the following:\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n   For VizDoom support: Install VizDoom\n   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n   `pip install vizdoomgym`.\n   For PyBullet support: `pip install pybullet`.\nb) To register your custom env, do `from ray import tune;\n   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n   Then in your config, do `config['env'] = [name]`.\nc) Make sure you provide a fully qualified classpath, e.g.:\n   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from trademaster.environments.portfolio_management.environment import PortfolioManagementEnvironment\n",
    "def env_creator(env_name):\n",
    "    if env_name == 'portfolio_management':\n",
    "        env = PortfolioManagementEnvironment\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return env\n",
    "# ray.init(ignore_reinit_error=True)\n",
    "# register_env(\"portfolio_management\", lambda config: env_creator(\"portfolio_management\")(config))\n",
    "trainer.test();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PortfolioManagementSARLTrainer' object has no attribute 'test_environment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot(trainer\u001b[39m.\u001b[39;49mtest_environment\u001b[39m.\u001b[39msave_asset_memory(),alg\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msarl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PortfolioManagementSARLTrainer' object has no attribute 'test_environment'"
     ]
    }
   ],
   "source": [
    "plot(trainer.test_environment.save_asset_memory(),alg=\"sarl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1f97403911abd3f02553c8f2b0c54537fddc7efadd9f5d3e31784db6e40c347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
